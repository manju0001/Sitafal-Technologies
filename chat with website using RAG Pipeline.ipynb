from sentence_transformers import SentenceTransformer
import numpy as np
import os
import warnings
from dotenv import load_dotenv
import requests
from bs4 import BeautifulSoup
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore
import faiss

# Load environment variables and suppress warnings
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
warnings.filterwarnings("ignore")
load_dotenv()

# Function to scrape website data with retries
def scrape_website(url, retries=3):
    for attempt in range(retries):
        try:
            response = requests.get(url, timeout=10)  # Add timeout to prevent indefinite waiting
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            # Extract main content
            main_content = soup.find('main')  # Modern sites often use <main> for primary content
            if main_content:
                return main_content.get_text(separator="\n", strip=True)
            else:
                return soup.get_text(separator="\n", strip=True)
        except requests.exceptions.RequestException as e:
            print(f"Attempt {attempt + 1} failed: {e}")
            if attempt == retries - 1:
                return ""

# Target website (Stanford University's "About" page)
website = "https://www.stanford.edu/about/"

# Scrape the website
content = scrape_website(website)
if not content:
    print("Failed to scrape the website. Exiting.")
    exit()

website_contents = [{"content": content, "metadata": {"url": website}}]

# Split content into chunks and convert to Document objects
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
document_chunks = []
for item in website_contents:
    raw_chunks = text_splitter.split_text(item["content"])
    document_chunks.extend([
        Document(page_content=chunk, metadata={"url": item["metadata"]["url"]})
        for chunk in raw_chunks
    ])

print(f"Number of chunks created: {len(document_chunks)}")

# Initialize embeddings using a fallback to SentenceTransformers
try:
    from langchain_ollama import OllamaEmbeddings
    embeddings = OllamaEmbeddings(model='nomic-embed-text')
    test_embedding = embeddings.embed_query("test")
    print("Using OllamaEmbeddings service.")
except Exception as e:
    print(f"Failed to initialize OllamaEmbeddings: {e}")
    print("Falling back to SentenceTransformers.")
    model = SentenceTransformer('all-MiniLM-L6-v2')  # Load a local model
    embeddings = lambda x: model.encode(x).tolist()  # Compatibility wrapper
    test_embedding = embeddings("test")

# FAISS setup
dimension = len(test_embedding)
vector_store = FAISS(
    embedding_function=embeddings,
    index=faiss.IndexFlatL2(dimension),
    docstore=InMemoryDocstore(),
    index_to_docstore_id={}
)

# Add documents to the vector store
ids = vector_store.add_documents(documents=document_chunks)
print(f"Documents added: {len(ids)}")

# Save the vector store
db_name = "website_embeddings"
vector_store.save_local(db_name)
print(f"Vector store saved as '{db_name}'.")

# Load the vector store for querying
try:
    new_vector_store = FAISS.load_local(db_name, embeddings=embeddings, allow_dangerous_deserialization=True)
except Exception as e:
    print(f"Failed to load vector store: {e}")
    exit()

# Query the vector store with dynamic user input
print("Enter questions about Stanford University ('exit' to quit):")
while True:
    question = input("> ").strip()
    if question.lower() == 'exit':
        print("Exiting.")
        break

    # Search for the question in the vector store
    try:
        docs = new_vector_store.search(query=question, search_type='similarity', k=1)  # Retrieve top result
        if docs:
            best_doc = max(docs, key=lambda doc: len(doc.page_content.split()))
            print(f"Results for Query: {question}")
            print(f"Source: {best_doc.metadata['url']}")
            print(best_doc.page_content)
            print("\n")
        else:
            print(f"No relevant information found for query: {question}")
    except Exception as e:
        print(f"Error during query: {e}")
